#!/usr/bin/env python3
"""
Regenerar base64_data para todas las im√°genes y optimizar rendimiento
"""
import asyncio
import asyncpg
import aiohttp
import base64
from io import BytesIO
from PIL import Image
import time

POSTGRES_URL = "postgresql://postgres:xhinRHxDvcdHNqyQKDTUbDKRLhYNLDum@ballast.proxy.rlwy.net:54363/railway"

async def optimize_image_for_base64(image_data, max_size=512):
    """Optimizar imagen para base64 manteniendo calidad visual"""
    try:
        # Abrir imagen
        img = Image.open(BytesIO(image_data))
        
        # Convertir a RGB si es necesario
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Redimensionar manteniendo aspect ratio
        img.thumbnail((max_size, max_size), Image.LANCZOS)
        
        # Guardar optimizada
        buffer = BytesIO()
        img.save(buffer, format='JPEG', quality=85, optimize=True)
        
        return buffer.getvalue()
    except Exception as e:
        print(f"    ‚ö†Ô∏è Error optimizando imagen: {e}")
        return image_data

async def download_and_convert_image(session, image_record):
    """Descargar imagen de Cloudinary y convertir a base64 optimizado"""
    
    image_id = image_record['id']
    filename = image_record['filename']
    cloudinary_url = image_record['cloudinary_url']
    
    print(f"  üì• Procesando: {filename[:50]}...")
    
    try:
        # Descargar imagen
        async with session.get(cloudinary_url) as response:
            if response.status == 200:
                image_data = await response.read()
                
                # Optimizar imagen
                optimized_data = await optimize_image_for_base64(image_data)
                
                # Convertir a base64
                base64_string = base64.b64encode(optimized_data).decode('utf-8')
                base64_data = f"data:image/jpeg;base64,{base64_string}"
                
                size_kb = len(optimized_data) / 1024
                print(f"    ‚úÖ Optimizada: {size_kb:.1f} KB")
                
                return {
                    'image_id': image_id,
                    'base64_data': base64_data,
                    'success': True
                }
            else:
                print(f"    ‚ùå Error HTTP {response.status}")
                return {'image_id': image_id, 'success': False, 'error': f'HTTP {response.status}'}
                
    except Exception as e:
        print(f"    ‚ùå Error: {e}")
        return {'image_id': image_id, 'success': False, 'error': str(e)}

async def regenerate_all_base64():
    """Regenerar base64_data para todas las im√°genes"""
    
    print("üöÄ INICIANDO REGENERACI√ìN DE BASE64")
    print("=" * 60)
    
    conn = await asyncpg.connect(POSTGRES_URL)
    
    try:
        # 1. Obtener todas las im√°genes sin base64
        print("üìã Obteniendo lista de im√°genes...")
        
        images = await conn.fetch("""
            SELECT id, filename, cloudinary_url
            FROM images 
            WHERE (base64_data IS NULL OR base64_data = '') 
            AND cloudinary_url IS NOT NULL
            ORDER BY filename
        """)
        
        total_images = len(images)
        print(f"   Total a procesar: {total_images}")
        
        if total_images == 0:
            print("‚úÖ Todas las im√°genes ya tienen base64")
            return
        
        # 2. Procesar en lotes para evitar sobrecarga
        print(f"\nüîÑ Procesando im√°genes...")
        
        start_time = time.time()
        successful = 0
        failed = 0
        
        # Crear sesi√≥n HTTP reutilizable
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            
            # Procesar en lotes de 5
            batch_size = 5
            
            for i in range(0, total_images, batch_size):
                batch = images[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_images + batch_size - 1) // batch_size
                
                print(f"\nüì¶ Lote {batch_num}/{total_batches} ({len(batch)} im√°genes):")
                
                # Procesar lote en paralelo
                tasks = [download_and_convert_image(session, img) for img in batch]
                results = await asyncio.gather(*tasks)
                
                # Guardar resultados exitosos
                updates = []
                for result in results:
                    if result['success']:
                        updates.append((result['base64_data'], result['image_id']))
                        successful += 1
                    else:
                        failed += 1
                        print(f"    ‚ùå Fall√≥ imagen ID: {result['image_id']}")
                
                # Actualizar en lote
                if updates:
                    await conn.executemany("""
                        UPDATE images 
                        SET base64_data = $1, updated_at = NOW()
                        WHERE id = $2
                    """, updates)
                    print(f"    üíæ Guardadas: {len(updates)} im√°genes")
                
                # Progreso
                processed = min(i + batch_size, total_images)
                elapsed = time.time() - start_time
                progress = (processed / total_images) * 100
                
                print(f"    üìä Progreso: {processed}/{total_images} ({progress:.1f}%) - {elapsed:.1f}s transcurridos")
                
                # Pausa breve entre lotes
                if batch_num < total_batches:
                    await asyncio.sleep(1)
        
        # 3. Resumen final
        total_time = time.time() - start_time
        
        print(f"\nüéØ REGENERACI√ìN COMPLETADA")
        print(f"=" * 60)
        print(f"‚úÖ Exitosas: {successful}")
        print(f"‚ùå Fallidas: {failed}")
        print(f"‚è±Ô∏è  Tiempo total: {total_time:.1f} segundos")
        print(f"‚ö° Promedio: {total_time/total_images:.2f}s por imagen")
        
        # 4. Verificar resultado final
        print(f"\nüîç VERIFICACI√ìN FINAL:")
        
        total_with_base64 = await conn.fetchval("""
            SELECT COUNT(*) FROM images 
            WHERE base64_data IS NOT NULL AND base64_data != ''
        """)
        
        total_images_db = await conn.fetchval("SELECT COUNT(*) FROM images")
        
        percentage = (total_with_base64 / total_images_db * 100) if total_images_db > 0 else 0
        
        print(f"   Im√°genes con base64: {total_with_base64}/{total_images_db} ({percentage:.1f}%)")
        
        if percentage == 100:
            print(f"\nüéâ ¬°OPTIMIZACI√ìN COMPLETA!")
            print(f"   El tiempo de respuesta ahora deber√≠a ser < 1 segundo")
        else:
            print(f"\n‚ö†Ô∏è  A√∫n faltan {total_images_db - total_with_base64} im√°genes")
        
        return {
            'successful': successful,
            'failed': failed,
            'total_time': total_time,
            'final_percentage': percentage
        }
        
    except Exception as e:
        print(f"‚ùå Error general: {e}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        await conn.close()

if __name__ == "__main__":
    print("üîß OPTIMIZADOR DE RENDIMIENTO CLIP")
    print("   Regenerando base64 para todas las im√°genes...")
    print()
    
    result = asyncio.run(regenerate_all_base64())
    
    if result:
        print(f"\n‚úÖ Proceso completado:")
        print(f"   - {result['successful']} im√°genes optimizadas")
        print(f"   - {result['failed']} errores")
        print(f"   - {result['total_time']:.1f} segundos total")
        print(f"   - {result['final_percentage']:.1f}% base64 completo")
        
        if result['final_percentage'] == 100:
            print(f"\nüöÄ ¬°SISTEMA OPTIMIZADO! Tiempo de respuesta mejorado significativamente")
    else:
        print("\nüí• Error en optimizaci√≥n")